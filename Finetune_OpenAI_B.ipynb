{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ypcio/GenAI_TMLC/blob/main/Finetune_OpenAI_B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install the required library\n",
        "\n",
        "Make sure you have the `openai` library installed. Use the command below if not already installed:"
      ],
      "metadata": {
        "id": "Rpi2JDlo2mLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==1.55.3 httpx==0.27.2"
      ],
      "metadata": {
        "id": "m4ZtKeih2ncx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68690115-21ae-430f-ded0-06bd2dddd6ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==1.55.3\n",
            "  Downloading openai-1.55.3-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting httpx==0.27.2\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.55.3) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.55.3) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.55.3) (0.8.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.55.3) (2.10.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.55.3) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.55.3) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai==1.55.3) (4.12.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.2) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.2) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.2) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx==0.27.2) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.55.3) (1.2.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.55.3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.55.3) (2.27.1)\n",
            "Downloading openai-1.55.3-py3-none-any.whl (389 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: httpx, openai\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.0\n",
            "    Uninstalling httpx-0.28.0:\n",
            "      Successfully uninstalled httpx-0.28.0\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.54.5\n",
            "    Uninstalling openai-1.54.5:\n",
            "      Successfully uninstalled openai-1.54.5\n",
            "Successfully installed httpx-0.27.2 openai-1.55.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "rKpH1ovI2pDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Access data from this [link](https://themlco-my.sharepoint.com/:f:/p/chiragchauhan/Eg7biBxP1hhEta6JLchrfWgBvLawQsfxSDJO6K1BQl_J9g?e=FhiWT3)"
      ],
      "metadata": {
        "id": "KSUn_jBPPcGM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Set up your API key\n",
        "Replace 'YOUR_API_KEY' with your actual OpenAI API key."
      ],
      "metadata": {
        "id": "p4AekMfl2rOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata # check the key shaped the symbol on the left pane of the notebook\n",
        "OpenAI_API = userdata.get('OpenAI')"
      ],
      "metadata": {
        "id": "G-zp8H3ZKVxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = OpenAI_API"
      ],
      "metadata": {
        "id": "f5fZB_Sd2sYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Prepare the dataset\n",
        "\n",
        "Hint: Use a JSON or CSV file and convert it to JSONL\n",
        "\n",
        "If loading a CSV dataset and convert it to JSONL format.\n",
        "Complete the conversion code below:\n",
        "\n",
        "Here for example directly using the training.jsonl file used during finetuning via playground"
      ],
      "metadata": {
        "id": "ZU9Lx5yu2uLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_csv('your_data.csv')  # Replace 'your_data.csv' with your dataset file\n",
        "\n",
        "# # Convert the dataset to JSONL format\n",
        "# output_file = 'data.jsonl'\n",
        "# with open(output_file, 'w') as f:\n",
        "#     for _, row in df.iterrows():\n",
        "#         # Create JSON lines for chat model fine-tuning\n",
        "#         json_line = json.dumps({\n",
        "#             \"messages\": [\n",
        "#                 {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "#                 {\"role\": \"user\", \"content\": row['question']},\n",
        "#                 {\"role\": \"assistant\", \"content\": row['answer']}\n",
        "#             ]\n",
        "#         })\n",
        "#         f.write(json_line + '\\n')\n",
        "# print(f\"Dataset converted and saved to {output_file}\")\n",
        "\n",
        "# comment when using your own data\n",
        "output_file = 'training.jsonl'"
      ],
      "metadata": {
        "id": "-B35fhFZ3ATh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Upload the file for fine-tuning\n",
        "\n",
        "Use the OpenAI API to upload the dataset. Replace '<JSONL_FILE>' with your dataset file name."
      ],
      "metadata": {
        "id": "e-PxneTJ2vtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "LT_MDQSx29pI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded_file = client.files.create(\n",
        "    file=open(output_file, \"rb\"),\n",
        "    purpose=\"fine-tune\"\n",
        ")\n",
        "print(f\"File uploaded successfully. File ID: {uploaded_file.id}\")"
      ],
      "metadata": {
        "id": "1nB1bEtf3D2Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe96f870-cdda-4cb8-c7d3-828d254f0a18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File uploaded successfully. File ID: file-H2eGyPY7TN8qbKKE1YCgnE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Fine-tune the model\n",
        "\n",
        "Trigger the fine-tuning job process using the uploaded file ID. Replace 'FILE_ID' and 'MODEL_NAME' accordingly."
      ],
      "metadata": {
        "id": "j-ASD4WO2xAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tune_job = client.fine_tuning.jobs.create(\n",
        "    training_file=uploaded_file.id,\n",
        "    suffix=\"custom-fine-tuned-model\",\n",
        "    model=\"gpt-4o-mini-2024-07-18\"  # Adjust the model as required\n",
        ")\n",
        "print(f\"Fine-tuning job started. Job ID: {fine_tune_job.id}\")"
      ],
      "metadata": {
        "id": "HROkLOlA3HH4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fbd9e9f-2dda-48d6-9d30-db03b164a8be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning job started. Job ID: ftjob-mv6DDWyq2YiWf1ECwWvUGqeN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Monitor and use the fine-tuned model\n",
        "\n",
        "Check list of fine-tuning jobs, retrieve job details."
      ],
      "metadata": {
        "id": "VADrU60f2yRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List fine-tuning jobs\n",
        "jobs = client.fine_tuning.jobs.list(limit=10)\n",
        "print(\"Recent fine-tuning jobs:\", jobs)"
      ],
      "metadata": {
        "id": "md147WBs3LAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09f55c50-106b-4118-853f-4884eb1699a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recent fine-tuning jobs: SyncCursorPage[FineTuningJob](data=[FineTuningJob(id='ftjob-mv6DDWyq2YiWf1ECwWvUGqeN', created_at=1733475531, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=1.8), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-FZKOY5vDxblsWFjjm6INeCk3', result_files=[], seed=1129603953, status='running', trained_tokens=None, training_file='file-H2eGyPY7TN8qbKKE1YCgnE', validation_file=None, estimated_finish=1733475769, integrations=[], user_provided_suffix='custom-fine-tuned-model'), FineTuningJob(id='ftjob-bKiVUjD3FoyftB7coZbkmj81', created_at=1733475433, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=1.8), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-FZKOY5vDxblsWFjjm6INeCk3', result_files=[], seed=245416096, status='cancelled', trained_tokens=None, training_file='file-H2eGyPY7TN8qbKKE1YCgnE', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None), FineTuningJob(id='ftjob-jIDQw4z7OEEnLReaCbkmR6Vk', created_at=1733471339, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-mini-2024-07-18:personal:genai-guided-projects-faq:AbNT4jAo', finished_at=1733471708, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=1.8), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-FZKOY5vDxblsWFjjm6INeCk3', result_files=['file-HDBsPNfWoFBegBsUkkTdGp'], seed=42, status='succeeded', trained_tokens=6078, training_file='file-8qF4fWdPTBFP56DmaWLWBP', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix='GenAI-Guided-Projects-FAQ')], object='list', has_more=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve job details\n",
        "job_details = client.fine_tuning.jobs.retrieve(fine_tune_job.id)\n",
        "print(\"Fine-tuning job details:\", job_details)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STiBDEfWNH_B",
        "outputId": "f1d81c99-ff51-4713-f0b4-2d5dc5b33809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning job details: FineTuningJob(id='ftjob-mv6DDWyq2YiWf1ECwWvUGqeN', created_at=1733475531, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=1.8), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-FZKOY5vDxblsWFjjm6INeCk3', result_files=[], seed=1129603953, status='running', trained_tokens=None, training_file='file-H2eGyPY7TN8qbKKE1YCgnE', validation_file=None, estimated_finish=1733475771, integrations=[], user_provided_suffix='custom-fine-tuned-model')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve after model is trained to get the model name\n",
        "job_details = client.fine_tuning.jobs.retrieve(fine_tune_job.id)\n",
        "print(\"Fine-tuning job details:\", job_details)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poRCrkKlOHdY",
        "outputId": "9e1e7bfd-b472-4333-fc82-491949ee7e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning job details: FineTuningJob(id='ftjob-mv6DDWyq2YiWf1ECwWvUGqeN', created_at=1733475531, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-mini-2024-07-18:personal:custom-fine-tuned-model:AbOWr1n9', finished_at=1733475787, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=1.8), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-FZKOY5vDxblsWFjjm6INeCk3', result_files=['file-3ynQi4zaj7y3eV6jxt3Lvm'], seed=1129603953, status='succeeded', trained_tokens=6078, training_file='file-H2eGyPY7TN8qbKKE1YCgnE', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix='custom-fine-tuned-model')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Use the fine-tuned model"
      ],
      "metadata": {
        "id": "QO4tPP_o2zgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example call to the fine-tuned model\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"ft:gpt-4o-mini-2024-07-18:personal:custom-fine-tuned-model:AbOWr1n9\",\n",
        "    # Replace with the actual model name retrieved in above cell fine_tuned_model='ft:gpt-4o-mini-2024-07-18:personal:custom-fine-tuned-model:AbOWr1n9'\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant which acts as FAQ Support Assistant for the TMLC Guided Projects in Generative AI Program and answer to user queries.\"},\n",
        "        {\"role\": \"user\", \"content\": \"How long is the program?\"}\n",
        "    ]\n",
        ")\n",
        "print(\"Fine-tuned model response:\", completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "Bh2TaZFo2tNJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5058ef25-0b0c-485e-aebe-9faf76714831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuned model response: This is a 6 weeks long program.\n"
          ]
        }
      ]
    }
  ]
}